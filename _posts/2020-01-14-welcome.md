# My very old notes from notepad

learning till now 

understood what is word2vec, tokenization, count vector, ld-idf etc etc


try many difficerent thing using these but didn't get any useful result yet 



above won't solve the problems we are trying to, these are just good models for preprocessed data


next need to try directly NN to solve the problem. 

don't think of logic is general sense. 
just try out NN and see what happens 


lets try to solve a basic issue first

a) based on cv's data classify cv in digial marketing or web developer profile 

b) based on cv's data classify into exp or non exp. for example. 

these are binary problem. first start with binary problem and then will go into multi classify

maybe try simple logistric regression as well

just try out MLPClassifier as well. to undertand it 

nexts steps
1) have read a lot of things in NLP, need to write it down to get everthing clear in the head


PCA, t-sine, https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html

2) see how we can improve our input data more. can we further classify cv data and provide our model much more 
relevent data to train on. e.g providing skills exp etc more??? need to think on this why our model accuracy is so low
3) need to look at feature extration from cv data etc or othing things like monkeylearn does 
4) look at doc2vec or sentence2dev
5) look at training our model with word embedding like word2vec or spacy instead of simple tfldf

6) use naukri google jos indeed etc as a source to train NN they have lot of different job descriptinos etc 


7) indeed looks promising for training on cv data https://resumes.indeed.com/search?l=Noida%2C%20Uttar%20Pradesh&q=Frontend%20Developer%20Javascript&rb=yoe%3A1-11  because they have already labdelled information

8) very important look at pre trained models like BERT etc for this

9) also need to look at text summry i.e summary of cv might also be used instead of reading full cv. with nlp 
also 

https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html

#lstm
https://machinelearningmastery.com/sequence-classification-lstm-recurrent-neural-networks-python-keras/
https://keras.io/examples/imdb_lstm/
https://adventuresinmachinelearning.com/keras-lstm-tutorial/
https://adventuresinmachinelearning.com/category/deep-learning/lstms/
https://colah.github.io/posts/2015-08-Understanding-LSTMs/
https://github.com/andreykurenkov/emailinsight/blob/master/pyScripts/kerasClassify.py
http://karpathy.github.io/2015/05/21/rnn-effectiveness/
https://nlpforhackers.io/keras-intro/







#clustering
https://ai.intelligentonlinetools.com/ml/text-clustering-word-embedding-machine-learning/


#to read latest blogs
https://www.analyticsvidhya.com/blog/2016/08/beginners-guide-to-topic-modeling-in-python/
https://www.analyticsvidhya.com/blog/2018/03/essentials-of-deep-learning-sequence-to-sequence-modelling-with-attention-part-i/?utm_source=blog&utm_medium=understanding-transformers-nlp-state-of-the-art-models
https://www.analyticsvidhya.com/blog/2019/06/understanding-transformers-nlp-state-of-the-art-models/?utm_source=blog&utm_medium=8-ambitious-data-science-projects-github
https://www.analyticsvidhya.com/blog/2019/03/pretrained-models-get-started-nlp/?utm_source=blog&utm_medium=8-ambitious-data-science-projects-github
https://www.analyticsvidhya.com/blog/2019/09/demystifying-bert-groundbreaking-nlp-framework/?utm_source=blog&utm_medium=8-ambitious-data-science-projects-github
https://www.analyticsvidhya.com/blog/2019/03/learn-to-use-elmo-to-extract-features-from-text/?utm_source=blog&utm_medium=8-ambitious-data-science-projects-github
https://www.analyticsvidhya.com/blog/2018/11/tutorial-text-classification-ulmfit-fastai-library/?utm_source=blog&utm_medium=8-ambitious-data-science-projects-github
https://www.analyticsvidhya.com/blog/2019/07/openai-gpt2-text-generator-python/?utm_source=blog&utm_medium=8-ambitious-data-science-projects-github

https://allennlp.org/tutor  ials
https://github.com/zalandoresearch/flair












https://toolbox.google.com/datasetsearch/search?query=Indeed.com%20job%20resume%20dataset.&docid=yize39BMPpdgLLFYAAAAAA%3D%3D

task which can be done
a) classify email based if its a candidate resume, or something like college, of agency etc or something else or just not important
b) classify unknow emails into job profile
c) look at linked msgs
d) look at google jobs etc i.e specific for top portals

cv shortling
a) see how to extract important features 
b) based on existing shortlisted cv maybe just find cosine similarty of vectors ?? intesting idea... experiment and see
e) see if we can actually parse cv via sencente classification multiple problems
f) need to also look at other factors like location(distance), prevous exp if related to industry or not, dob how old. etc these are factors which get missed in general shortlist by HR. but machines should be good at this  
g) i think skills etc need to look at from word2vec point of view

look at CRF and knowledge graph to extract information https://github.com/Jiakui/awesome-bert#bert--knowledge-graph-task-






# start on autoencoders it worked well for images to an extened but given much for text
# need to go back to it again and look at deep clustring as well. 
# need to also understand in deep about lstm with attention in language modelling and it will give 
# more insights into embedding and NN models
# also need to understand more on if we using word embedding do we need to use LSTM again or we can just 
# use CNN as we already have good word embeddings now.
# anyways will come back later to this agian



# unsupoervised learning i.e auto encoder seems easier because we don't need much pre-made data. but 
# results are not very good with unsupervised leanring. as in auto encoders. 

<!-- can work on semantic search as well and recommendation engine -->
<!-- ofcouse chatsbot for ecommerce as well -->

jupyter serverextension enable --py jupyter_http_over_ws

jupyter notebook   --NotebookApp.allow_origin='https://colab.research.google.com'   --port=8888   --NotebookApp.port_retries=0 --JupyterWebsocketPersonality.list_kernels=True --NotebookApp.disable_check_xsrf=True --no-browser --NotebookApp.password='' --no-mathjax &


ssh -nNT -L 8888:localhost:8888 node@176.9.137.77 -p 7002

https://drive.google.com/open?id=1adiBcvyhxSPMRSa_u6RwHwECzGUQEcLA

wget --load-cookies /tmp/cookies.txt "https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1adiBcvyhxSPMRSa_u6RwHwECzGUQEcLA' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\1\n/p')&id=1adiBcvyhxSPMRSa_u6RwHwECzGUQEcLA" -O dataset.pth && rm -rf /tmp/cookies.txt


gcloud compute ssh --project java-ref --zone us-west1-b torch2-vm -- -L 8080:localhost:8080

gcloud compute project-info add-metadata \
    --metadata enable-oslogin=TRUE
# all os login to all instances

gcloud beta compute instances create torchvm3 \
  --zone=us-central1-f \
  --image-family=pytorch-latest-gpu \
  --image-project=deeplearning-platform-release \
  --machine-type="n1-standard-1" \
  --scopes storage-rw


gcloud beta compute instances create torchvm3 \
  --zone=us-central1-a \
  --image-family=pytorch-latest-gpu \
  --image-project=deeplearning-platform-release \
  --maintenance-policy=TERMINATE \
  --accelerator="type=nvidia-tesla-t4,count=1" \
  --metadata="install-nvidia-driver=True" \
  --machine-type="n1-standard-4" \
  --scopes storage-rw \
  --preemptible

#n1 standard 4 


gcloud compute instances attach-disk torchvm3 --disk torch --zone us-central1-f

gcloud beta compute ssh --project java-ref --zone us-central1-a torchvm3 -- -L 8080:localhost:8080

sudo lsblk
sudo mkfs.ext4 -m 0 -F -E lazy_itable_init=0,lazy_journal_init=0,discard /dev/sdb



sudo mkdir -p /home/jupyter/drive
sudo mount -o discard,defaults /dev/sdb /home/jupyter/drive
sudo chmod a+w /home/jupyter/drive

gcloud compute instances stop torchvm3 --zone=us-central1-a
gcloud compute instances delete torchvm3 --zone=us-central1-a


gcloud auth login
gcloud config set project recruitai-266705
gsutil ls
gsutil -m cp -r recruit-ner-bert-flair-augment/ gs://recruitaiwork/recruit-ner-flair-augment

https://explosion.ai/blog/sense2vec-reloaded



===nlp where i left it and general nlp ===
https://github.com/napsternxg/pytorch-practice
https://github.com/deepmipt/DeepPavlov
https://github.com/asyml/texar
https://github.com/brightmart/text_classification
https://github.com/kk7nc/Text_Classification
https://github.com/ThilinaRajapakse/pytorch-transformers-classification/blob/master/colab_quickstart.ipynb
https://github.com/UKPLab/sentence-transformers#pretrained-models
https://towardsdatascience.com/news-to-company-linking-with-bert-48a1ac9805f1
https://github.com/facebookresearch/faiss/wiki/Getting-started
https://github.com/kaushaltrivedi/bert-toxic-comments-multilabel/blob/master/toxic-bert-multilabel-classification.ipynb?source=post_page-----69714fa3fb3d----------------------
https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html
https://github.com/pytorch/fairseq/tree/master/examples/roberta
https://github.com/nlpyang/BertSum/blob/master/src/models/model_builder.py

https://mlexplained.com/2019/01/07/paper-dissected-bert-pre-training-of-deep-bidirectional-transformers-for-language-understanding-explained/

https://colab.research.google.com/drive/1Xph-1GLUf4BRzCD9UXWY7EphRi2t9cBj#scrollTo=knxB1a4K4W2z
https://github.com/santhoshkolloju/Abstractive-Summarization-With-Transfer-Learning
https://github.com/lonePatient/Bert-Multi-Label-Text-Classification/blob/master/run_xlnet.py

https://mccormickml.com/2019/09/19/XLNet-fine-tuning/
https://mccormickml.com/2019/07/22/BERT-fine-tuning/



=== auto encoder where i left it ===
https://github.com/krasserm/bayesian-machine-learning
https://github.com/vlukiyanov/pt-dec
https://github.com/wiseodd/generative-models/tree/master/VAE
https://github.com/1Konny/Beta-VAE/blob/master/solver.py
https://github.com/YannDubs/disentangling-vae
https://github.com/YannDubs/disentangling-vae/blob/master/hyperparam.ini
https://github.com/1Konny/FactorVAE/blob/master/solver.py
https://arxiv.org/pdf/1804.03599.pdf
https://towardsdatascience.com/what-a-disentangled-net-we-weave-representation-learning-in-vaes-pt-1-9e5dbc205bd1
https://colab.research.google.com/drive/1BIhjBwN0WpqhI8Qd4hQ7-KPugHxvMKp-#scrollTo=M0d3cRvRq1my
https://colab.research.google.com/drive/18hWnKeT7KRhcws2J6QSKiMulHTa9oywa#scrollTo=uEELS6gucvXB

https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html


=== going deep into NER specially bi-lstm CRF ===

https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html
https://towardsdatascience.com/conditional-random-field-tutorial-in-pytorch-ca0d04499463
https://pytorch.org/tutorials/beginner/nlp/advanced_tutorial.html
https://prateekvjoshi.com/2013/02/23/what-are-conditional-random-fields/
https://prateekvjoshi.com/2013/02/23/why-do-we-need-conditional-random-fields/
https://medium.com/@postsanjay/hidden-markov-models-simplified-c3f58728caab
https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Sequence-Labeling/
https://github.com/LiyuanLucasLiu/LM-LSTM-CRF
https://colab.research.google.com/drive/1xRtdAE72aUxUDhMQjtmyxraVZmSR-gxB#scrollTo=R1wPB8OxN-tt
https://github.com/ZhixiuYe/HSCRF-pytorch



https://github.com/huggingface/transformers/issues/876



==== search in vector space https://github.com/facebookresearch/faiss ===


https://github.com/facebookresearch/InferSent/blob/master/demo.ipynb
https://medium.com/huggingface/universal-word-sentence-embeddings-ce48ddc8fc3a
https://github.com/UKPLab/arxiv2018-xling-sentence-embeddings
https://blog.onebar.io/building-a-semantic-search-engine-using-open-source-components-e15af5ed7885



competition 
https://cutshort.io/pricing
https://cutshort.io/voila
https://www.instahyre.com/


https://recooty.com/

design of zety.com looks good